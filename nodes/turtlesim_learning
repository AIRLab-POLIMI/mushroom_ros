#!/usr/bin/env python3

import numpy as np

from mushroom_rl_ros.environments import TurtleSim

from mushroom_rl.core import Core
from mushroom_rl.algorithms.policy_search import RWR
from mushroom_rl.approximators.parametric import LinearApproximator
from mushroom_rl.approximators.regressor import Regressor
from mushroom_rl.features.features import Features
from mushroom_rl.features.tiles import Tiles
from mushroom_rl.policy import DeterministicPolicy
from mushroom_rl.distributions import GaussianDiagonalDistribution
from mushroom_rl.utils.dataset import compute_J

# Learning parameters
n_runs = 4
n_iterations = 10
ep_per_run = 3

# Environment
mdp = TurtleSim()

# Policy
tiles = Tiles.generate(3, [5]*3, mdp.info.observation_space.low,
                       mdp.info.observation_space.high)

phi = Features(tilings=tiles)

input_shape = (phi.size,)

approximator_params = dict(input_dim=phi.size)
approximator = Regressor(LinearApproximator, input_shape=input_shape,
                         output_shape=mdp.info.action_space.shape,
                         params=approximator_params)

policy = DeterministicPolicy(approximator)

mu = np.zeros(policy.weights_size)
std = 0.1*np.ones(policy.weights_size)
dist = GaussianDiagonalDistribution(mu, std)

# Agent
beta = 1.0
agent = RWR(mdp.info, dist, policy, beta, phi)

# Train
core = Core(agent, mdp)
print('Initial evaluation')
dataset_eval = core.evaluate(n_episodes=ep_per_run)
J = compute_J(dataset_eval, gamma=mdp.info.gamma)
print('J at start : ' + str(np.mean(J)))

for i in range(n_runs):
    print('iteration', i)
    print('learn')
    core.learn(n_episodes=n_iterations * ep_per_run,
               n_episodes_per_fit=ep_per_run)
    print('evaluate')
    dataset_eval = core.evaluate(n_episodes=ep_per_run)
    J = compute_J(dataset_eval, gamma=mdp.info.gamma)
    print('J at iteration ' + str(i) + ': ' + str(np.mean(J)))

mdp.stop()

